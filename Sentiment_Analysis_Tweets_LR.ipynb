{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Sentiment_Analysis_Tweets_LR.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyNTwPZev/85ngf/xLL+iFSo",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ANR22/New_repo/blob/main/Sentiment_Analysis_Tweets_LR.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "83VD-ewIGSU3"
      },
      "source": [
        "import nltk\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from nltk.corpus import twitter_samples\n",
        "import re                                  # library for regular expression operations\n",
        "import string                              # for string operations\n",
        "\n",
        "from nltk.corpus import stopwords          # module for stop words that come with NLTK\n",
        "from nltk.stem import PorterStemmer        # module for stemming\n",
        "from nltk.tokenize import TweetTokenizer   # module for tokenizing strings"
      ],
      "execution_count": 39,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "n43D0HTqTmi6",
        "outputId": "677cf816-40d6-42f9-aa1a-0273aac6852d"
      },
      "source": [
        "nltk.download('twitter_samples')\n",
        "nltk.download('stopwords')"
      ],
      "execution_count": 40,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[nltk_data] Downloading package twitter_samples to /root/nltk_data...\n",
            "[nltk_data]   Package twitter_samples is already up-to-date!\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 40
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Fdf7OJTkH7aJ"
      },
      "source": [
        "def process_tweet(tweet):\n",
        "    \"\"\"Process tweet function.\n",
        "    Input:\n",
        "        tweet: a string containing a tweet\n",
        "    Output:\n",
        "        tweets_clean: a list of words containing the processed tweet\n",
        "\n",
        "    \"\"\"\n",
        "    stemmer = PorterStemmer()\n",
        "    stopwords_english = stopwords.words('english')\n",
        "    # remove stock market tickers like $GE\n",
        "    tweet = re.sub(r'\\$\\w*', '', tweet)\n",
        "    # remove old style retweet text \"RT\"\n",
        "    tweet = re.sub(r'^RT[\\s]+', '', tweet)\n",
        "    # remove hyperlinks\n",
        "    tweet = re.sub(r'https?:\\/\\/.*[\\r\\n]*', '', tweet)\n",
        "    # remove hashtags\n",
        "    # only removing the hash # sign from the word\n",
        "    tweet = re.sub(r'#', '', tweet)\n",
        "    # tokenize tweets\n",
        "    tokenizer = TweetTokenizer(preserve_case=False, strip_handles=True,\n",
        "                               reduce_len=True)\n",
        "    tweet_tokens = tokenizer.tokenize(tweet)\n",
        "\n",
        "    tweets_clean = []\n",
        "    for word in tweet_tokens:\n",
        "        if (word not in stopwords_english and  # remove stopwords\n",
        "                word not in string.punctuation):  # remove punctuation\n",
        "            # tweets_clean.append(word)\n",
        "            stem_word = stemmer.stem(word)  # stemming word\n",
        "            tweets_clean.append(stem_word)\n",
        "\n",
        "    return tweets_clean\n",
        "\n",
        "\n",
        "def build_freqs(tweets, ys):\n",
        "    \"\"\"Build frequencies.\n",
        "    Input:\n",
        "        tweets: a list of tweets\n",
        "        ys: an m x 1 array with the sentiment label of each tweet\n",
        "            (either 0 or 1)\n",
        "    Output:\n",
        "        freqs: a dictionary mapping each (word, sentiment) pair to its\n",
        "        frequency\n",
        "    \"\"\"\n",
        "    yslist = np.squeeze(ys).tolist()\n",
        "    freqs = {}\n",
        "    for y, tweet in zip(yslist, tweets):\n",
        "        for word in process_tweet(tweet):\n",
        "            pair = (word, y)\n",
        "            if pair in freqs:\n",
        "                freqs[pair] += 1\n",
        "            else:\n",
        "                freqs[pair] = 1\n",
        "\n",
        "    return freqs\n"
      ],
      "execution_count": 41,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IUEGk513NJCs"
      },
      "source": [
        "\n",
        "all_positive_tweets = twitter_samples.strings(\"positive_tweets.json\")\n",
        "all_negative_tweets = twitter_samples.strings(\"negative_tweets.json\")"
      ],
      "execution_count": 42,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nVWMz7O5N_S_",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "866ba45b-deef-4c37-aae7-bc4bd1eeb70f"
      },
      "source": [
        "test_pos=all_positive_tweets[4000:]\n",
        "train_pos = all_positive_tweets[0:4000]\n",
        "test_neg = all_negative_tweets[4000:]\n",
        "train_neg = all_negative_tweets[0:4000]\n",
        "\n",
        "train_x = train_pos + train_neg\n",
        "test_x = test_pos + test_neg\n",
        "type(train_x)"
      ],
      "execution_count": 43,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "list"
            ]
          },
          "metadata": {},
          "execution_count": 43
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ft2mOypWOm1t"
      },
      "source": [
        "train_y = np.append(np.ones((len(train_pos),1)) , np.zeros((len(train_neg),1)), axis=0)\n",
        "test_y = np.append(np.ones((len(test_pos),1)) , np.zeros((len(test_neg),1)), axis=0)"
      ],
      "execution_count": 44,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CP_a9gUWPdfX",
        "outputId": "76b5aece-0a61-48bb-b646-a6a3650edd9d"
      },
      "source": [
        "print(\"train_y.shape=\",str(train_y.shape))\n",
        "print(\"test_y.shape=\",str(test_y.shape))"
      ],
      "execution_count": 45,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "train_y.shape= (8000, 1)\n",
            "test_y.shape= (2000, 1)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_a6K7Ai7Px5P",
        "outputId": "1b30d147-b47c-4897-a01a-c23aeddc9e88"
      },
      "source": [
        "freqs = build_freqs(train_x,train_y)\n",
        "print(\"type(freqs) =\",str(type(freqs)))\n",
        "print(\"len(frqs) =\",str(len(freqs.keys())))"
      ],
      "execution_count": 46,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "type(freqs) = <class 'dict'>\n",
            "len(frqs) = 11346\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "m-IEm7bja3WB"
      },
      "source": [
        "def sigmoid(z): \n",
        "    '''\n",
        "    Input:\n",
        "        z: is the input (can be a scalar or an array)\n",
        "    Output:\n",
        "        h: the sigmoid of z\n",
        "    '''\n",
        "    z=np.multiply(z,-1)\n",
        "    h = 1/(1+np.exp(z))\n",
        "\n",
        "    return h"
      ],
      "execution_count": 47,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4RD1s1UAa9jx"
      },
      "source": [
        "def gradientDescent(x, y, theta, alpha, num_iters):\n",
        "    '''\n",
        "    Input:\n",
        "        x: matrix of features which is (m,n+1)\n",
        "        y: corresponding labels of the input matrix x, dimensions (m,1)\n",
        "        theta: weight vector of dimension (n+1,1)\n",
        "        alpha: learning rate\n",
        "        num_iters: number of iterations you want to train your model for\n",
        "    Output:\n",
        "        J: the final cost\n",
        "        theta: your final weight vector\n",
        "    '''\n",
        "    m = x.shape[0]\n",
        "    \n",
        "    for i in range(0, num_iters):\n",
        "        z = np.dot(x,theta)\n",
        "        h = sigmoid(z)\n",
        "        \n",
        "        # calculate the cost function\n",
        "        J = (-1./m)*(np.dot(np.transpose(y),np.log(h)) + np.dot(np.transpose(1-y),np.log(1-h)))\n",
        "        \n",
        "        # update the weights theta\n",
        "        theta = theta-(alpha/m)*(np.dot(np.transpose(x),(h-y)))\n",
        "        \n",
        "    \n",
        "    J = float(J)\n",
        "    return J, theta"
      ],
      "execution_count": 48,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wJ5VAlUvl9_y"
      },
      "source": [
        "def extract_features(tweet, freqs):\n",
        "    '''\n",
        "    Input: \n",
        "        tweet: a list of words for one tweet\n",
        "        freqs: a dictionary corresponding to the frequencies of each tuple (word, label)\n",
        "    Output: \n",
        "        x: a feature vector of dimension (1,3)\n",
        "    '''\n",
        "    word_l = process_tweet(tweet)\n",
        "    x = np.zeros((1, 3)) \n",
        "    x[0,0] = 1 \n",
        "\n",
        "    for word in word_l:\n",
        "        \n",
        "        if (word,1) in freqs:\n",
        "            x[0,1] += freqs[(word,1)]\n",
        "        \n",
        "        if (word,0) in freqs:\n",
        "            x[0,2] += freqs[(word,0)]\n",
        "        \n",
        "    assert(x.shape == (1, 3))\n",
        "    return x"
      ],
      "execution_count": 49,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LE2IOosTmOcv",
        "outputId": "1f8de796-e3b6-4058-a4a6-635856adfe79"
      },
      "source": [
        "X = np.zeros((len(train_x), 3))\n",
        "for i in range(len(train_x)):\n",
        "    X[i, :]= extract_features(train_x[i], freqs)\n",
        "    \n",
        "Y = train_y\n",
        "\n",
        "# Apply gradient descent\n",
        "J, theta = gradientDescent(X, Y, np.zeros((3, 1)), 1e-9, 1500)\n",
        "print(f\"The cost after training is {J:.8f}.\")\n",
        "print(f\"The resulting vector of weights is {[round(t, 8) for t in np.squeeze(theta)]}\")"
      ],
      "execution_count": 50,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The cost after training is 0.24216529.\n",
            "The resulting vector of weights is [7e-08, 0.0005239, -0.00055517]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3VKsX2KJoOmP"
      },
      "source": [
        "\n",
        "def predict_tweet(tweet, freqs, theta):\n",
        "    '''\n",
        "    Input: \n",
        "        tweet: a string\n",
        "        freqs: a dictionary corresponding to the frequencies of each tuple (word, label)\n",
        "        theta: (3,1) vector of weights\n",
        "    Output: \n",
        "        y_pred: the probability of a tweet being positive or negative\n",
        "    '''\n",
        "    x = extract_features(tweet, freqs)\n",
        "    \n",
        "\n",
        "    y_pred = sigmoid(np.dot(x,theta))\n",
        "    \n",
        "    \n",
        "    return y_pred"
      ],
      "execution_count": 51,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8HUbPSWloSDf",
        "outputId": "d5fcf1cb-991c-4734-bbfc-22c69c0dba1e"
      },
      "source": [
        "for tweet in ['I am happy', 'I am bad', 'this movie should have been great.', 'great', 'great great', 'great great great', 'great great great great']:\n",
        "    print( '%s -> %f' % (tweet, predict_tweet(tweet, freqs, theta)))"
      ],
      "execution_count": 52,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "I am happy -> 0.518580\n",
            "I am bad -> 0.494339\n",
            "this movie should have been great. -> 0.515331\n",
            "great -> 0.515464\n",
            "great great -> 0.530898\n",
            "great great great -> 0.546273\n",
            "great great great great -> 0.561561\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cVKoZHY4rK0u"
      },
      "source": [
        "def test_logistic_regression(test_x, test_y, freqs, theta):\n",
        "    \"\"\"\n",
        "    Input: \n",
        "        test_x: a list of tweets\n",
        "        test_y: (m, 1) vector with the corresponding labels for the list of tweets\n",
        "        freqs: a dictionary with the frequency of each pair (or tuple)\n",
        "        theta: weight vector of dimension (3, 1)\n",
        "    Output: \n",
        "        accuracy: (# of tweets classified correctly) / (total # of tweets)\n",
        "    \"\"\"\n",
        "    \n",
        "    y_hat = []\n",
        "    \n",
        "    for tweet in test_x:\n",
        "        \n",
        "        y_pred = predict_tweet(tweet, freqs, theta)\n",
        "        \n",
        "        if y_pred > 0.5:\n",
        "            y_hat.append(1)\n",
        "        else:\n",
        "            y_hat.append(0)\n",
        "\n",
        "    m=test_y.shape[0]\n",
        "    y_hat=np.array(y_hat)\n",
        "    y_hat = np.resize(y_hat,(m,1))\n",
        "    accuracy = (np.sum(y_hat==test_y))/m\n",
        "\n",
        "    \n",
        "    return accuracy"
      ],
      "execution_count": 53,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZzZGV_5NrL4L",
        "outputId": "76c51884-db2a-4f08-ac6b-483e91ccd4f5"
      },
      "source": [
        "tmp_accuracy = test_logistic_regression(test_x, test_y, freqs, theta)\n",
        "print(f\"Logistic regression model's accuracy = {tmp_accuracy:.4f}\")"
      ],
      "execution_count": 54,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Logistic regression model's accuracy = 0.9950\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vBeXTcXXsTga",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "bd9b468d-0a1b-431a-eca9-55154b61a9e6"
      },
      "source": [
        "my_tweet = '\"If you have several dozen or several hundred contacts, then imagine the fun of sending each of them one by one.\"Tied to charger for conversations lasting more than 45 minutes.MAJOR PROBLEMS!!'\n",
        "print(process_tweet(my_tweet))\n",
        "y_hat = predict_tweet(my_tweet, freqs, theta)\n",
        "print(y_hat)\n",
        "if y_hat > 0.5:\n",
        "    print('Positive sentiment')\n",
        "else: \n",
        "    print('Negative sentiment')"
      ],
      "execution_count": 55,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['sever', 'dozen', 'sever', 'hundr', 'contact', 'imagin', 'fun', 'send', 'one', 'one', 'tie', 'charger', 'convers', 'last', '45', 'minutes.major', 'problem']\n",
            "[[0.49513105]]\n",
            "Negative sentiment\n"
          ]
        }
      ]
    }
  ]
}